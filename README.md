# CSE543_Final_Project_TKT
## Introduction
This project focuses on implementing a Transformer for Knowledge Tracing (KT) problems on the basis of the SAINT model (https://github.com/Shivanandmn/Knowledge-Tracing-SAINT). We adapted the input embedding and the model, and then tested the model on the ASSISTments2017 dataset. Compared to other transformer models' performance on ASSISTments2017 (AUC=0.7181), our model can increase the AUC by 2% (AUC=0.7301).

## Dataset
### ASSISTment2017
### Riiid

## References
[1] https://github.com/Shivanandmn/Knowledge-Tracing-SAINT.

[2] Piech, C., Bassen, J., Huang, J., Ganguli, S., Sahami, M., Guibas, L. J., & Sohl-Dickstein, J. (2015). Deep knowledge tracing. Advances in neural information processing systems, 28.

[3] Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., ... & Polosukhin, I. (2017). Attention is all you need. Advances in neural information processing systems, 30.

[4] Pu, S., Yudelson, M., Ou, L., & Huang, Y. (2020, July). Deep knowledge tracing with transformers. In International Conference on Artificial Intelligence in Education (pp. 252-256). Springer, Cham.

[5] Choi, Y., Lee, Y., Cho, J., Baek, J., Kim, B., Cha, Y., ... & Heo, J. (2020, August). Towards an appropriate query, key, and value computation for knowledge tracing. In Proceedings of the Seventh ACM Conference on Learning@ Scale (pp. 341-344).

[6] Janizek, J. D., Sturmfels, P., & Lee, S. I. (2021). Explaining explanations: Axiomatic feature interactions for deep networks. Journal of Machine Learning Research, 22(104), 1-54.
